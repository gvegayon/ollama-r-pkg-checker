---
title: "Getting started with Ollama in Python"
format: gfm
author: George G Vega Yon
date: 2025-10-01
jupyter: python3
---

This document shows how we can use the `ollama` Python package to interact with Ollama models. To run this, it is assumed that you have the ollama server up and running on your local machine (you started `ollama serve` in your terminal), and that you have installed the `ollama` Python package. 

You can learn more about ollama and the Python package in the [official documentation](https://ollama.com/docs).

## Building the chat as a model

One nice feature of the ollama interface is that we can pass the entire conversation history to the model to use it as context. This is done by passing the previous messages as a list of dictionaries to the `messages` argument of the `client.chat()` method.

The following function, `tell_me_about_r()` is a simple example using the `client.chat()` method to have a conversation with the model about R programming. The function makes three calls to the `client.chat()` method, each time passing the previous messages as context. We will use this as a way to compare the performance of three different models: `deepseek-r1:1.5b`, `tinyllama:1.1b`, and `qwen2.5-coder:1.5b` (all lightweight and easy to run on CPUs).

To ensure reproducibility, we set the `seed` option to a fixed value. We also set the `temperature` option to 0 to make the model's responses more deterministic.

```{python}
#| label: build-model
import ollama

def tell_me_about_r(
    model: str,
    options: dict = { "seed": 124, "temperature": 0 }
    ) -> None:

    # Function to print the output
    def print_response(response: dict, query: str) -> None:
        print(f'\n{"-" * 80}\n[Model: {model}] Response time: {response["total_duration"] / 1e9} seconds\n{"-" * 80}')
        print(f'Response to the query:\n{query}\n{"-" * 80}')
        print(response['message']['content'])
        return None

    # Starting the client
    client = ollama.Client()

    # First response
    response = client.chat(
        model=model,
        messages=[
            {'role': 'user', 'content': "Can you tell me about R programming?",}
        ],
        think=False, # gemma3 doesn't support think
        options=options
    )

    # Printing response 1
    print_response(response, "Can you tell me about R programming?")

    # Second response
    response2 = client.chat(
        model=model,
        messages=[
            *[response['message']],
            {'role': 'user', 'content': 'What are the main differences it has with C++? I am thinking of language features that would typically be standard in one and not the other',}
        ],
        think=False,
        options=options
    )

    # Printing response 2
    print_response(response2, 'What are the main differences it has with C++? I am thinking of language features that would typically be standard in one and not the other')

    # Third response
    response3 = client.chat(
        model=model,
        messages=[
            *[response['message']],
            *[response2['message']],
            {'role': 'user', 'content': 'Summarize our conversation so far in a few sentences.',}
        ],
        think=False,
        options=options
    )

    # Printing response 3
    print_response(response3, "Summarize our conversation so far in a few sentences.")

    return None
```

## Example: Deepseek-r1:1.5b

```{python}
#| label: deepseek-r1:1.5b
tell_me_about_r(model = 'deepseek-r1:1.5b')
```

## Example: tinyllama:1.1b

```{python}
#| label: tinyllama:1.1b
tell_me_about_r(model = 'tinyllama:1.1b')
```

## Example: qwen2.5-coder:1.5b

```{python}
#| label: qwen2.5-coder:1.5b
tell_me_about_r(model = 'qwen2.5-coder:1.5b')
```

